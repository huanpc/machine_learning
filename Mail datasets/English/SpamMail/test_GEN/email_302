<message><date> thu, 01 may 2003 16:48:09 +0100 </date><to> ac.uk </to><from> ac.uk </from><subject><text_normal> ^ re : rasp subcat lexicon </text_normal></subject><content-type> text/plain; charset="us-ascii" </content-type><message_body><text_embedded> ^ there are  um verbs in  ame , so it wld be interesting to know how well these map to the frequency ranking from  ame -- maybe we shld use comlex as our list of verbs to build entries for ? </text_embedded><text_normal> ^ yes , that would also enable me to use consistent method for lexicon building / filtering . ^ we may want to omit some of the low frequency comlex verbs , though ... i made some calculations . ^ there 're now  um new files in /usr / groups / dict / subcat / rasp_lexicon ^  um ) in-comlex ^ which contains  ame frequencies for the  um comlex verbs :  um verbs occur more than  um times  um verbs occur less than  um times ^  um ) not-comlex ^ which contains frequencies for the  um ,  um non-comlex verbs in  ame :  um verbs occur more than  um times ( most are  ame spellings or noise from the tagger / parser )  um ,  um verbs occur less than  um times </text_normal><text_embedded> ^ there are another  um which occur btwn  um times and  um which occur  um times -- how do we know that  um is the correct cut-off ? ^ maybe we shld be thinking more in terms of  um egs / verb so that we can allow for parse failures etc ? </text_embedded><text_normal> ^ yes , i meant  um occurrences after parsing ( i.e. after failures ) . ^ the above freqs were calculated from parsed data . ^ to end up with  um , we 'd need some  um egs / verb as input to parsing . ^ that 's really the minimum . ^ the accuracy of both subcat acquisition and clustering drops sharply if we use less than  um parsed sentences ( not sure about wsd / sel . ^ prefs , but i 'd imagine it 's the same ) . ^ generally , of course , the more data the better . ^ in my recent experiments , i got best subcat and clustering results with my biggest dataset --  um egs per verb . ^ for the * really * frequent verbs , should we use all the data we have in  ame , or do we set some upper bound ? </text_normal><text_embedded> ^ perhaps this would be a sensible way to proceed : ^ use some correlation of the  ame ranking and comlex to choose 5-6k verbs that we shld do ( i.e. add non-comlex verbs that occur more than  um times in  ame ^ get all the relevant sentences from the  ame and ( re ) parse them and extract patternsets ^ get more sentences for those verbs underrepresented in  ame ( (  um egs ? ) by creating queries to  ame designed to find pdf / ps documents containing them , feed these sentences through pdftotext / pstoascii some special preprocessing , and then rasp ^ i think i can set up the pipeline to parse and extract patternsets ) , but it wld be good to check manually with  ame that we 'll find enough relevant sentences this way for a small sample of underrepresented candidates ... ^ proceed as before ^ any thoughts ? </text_embedded><text_normal> ^ this sounds good . ^ i can do the manual checking for  um frequency ranges of ( samples of ) underrepresented verbs e.g. by next week . ^  ame </text_normal></message_body></message>
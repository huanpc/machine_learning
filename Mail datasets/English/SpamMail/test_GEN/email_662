<message><date> fri, 11 apr 2003 14:20:47 +0100 </date><from> ac.uk </from><to> edu </to><subject><text_normal> ^ re : multilingual text categorisation </text_normal></subject><content-type> text/plain; charset=iso-8859-1 </content-type><message_body><text_normal> ^ both language modeling papers i listed mentioned a simple technique for obtaining training data using a standard search engine such as  ame i.e. you can simply specify the language of the document to be searched for . ^  ame ^  ame  ame  ame (  mail ) : </text_normal><text_embedded> ^ by ' multilingual  ame  ame ' i mean the problem of automatically assigning a category to a document whatever it is the language . ^ the suggested approach , build separate language models for the different languages , supposes to have a sufficient quantity of texts in each language ( and for each category ) to be able to learn a model . ^ in the real life this is not always true . ^  ame , ^ at  um :  um  um /  um /  um , you wrote : <text_embedded> ^ i suppose if you meant applying text categorization methods to a multi-lingual set of documents , then you could just build separate language models for the different languages using the same approach as mentioned in the above papers . ^  ame  ame school of  ame university of  ame ,  ame ^  ame  ame  ame (  mail ) : <text_embedded> ^  ame  ame ^ i would be most grateful if anyone had any references on  ame  ame  ame . ^ thank you for your help </text_embedded></text_embedded></text_embedded><text_normal> ^ school of informatics , university of  ame ,  ame this mail sent through  ame :  ebsite </text_normal></message_body></message>
<message>
<date> tue, 28 jan 2003 17:02:29 +0000 (gmt) </date>
<from> ac.uk </from>
<to> ac.uk </to>
<subject>
<text_normal> 

^ &name practical 
</text_normal>
</subject>
<content-type> text/plain; charset=us-ascii </content-type>
<message_body>
<text_normal> 

^ hi &name , 
^ just a couple of questions about the speech practical , if you have time : 
^ ( i have completed my recogniser , and compared it with the original . 
^ it appears the original ( &name ) has a default insertion penalty of &num , but otherwise they give similar results ) 
^ having done a number of tests using various &name tools to train the &name 's, i obtained the following results : 
^ it appears that &name performs a certain amount of re-estimation by default , which i reduced to a single iteration using the -n &num flag . 
^ this gave me an initial error rate on the training and test data . 
^ using &name and allowing the algorithm to converge ( the number of iterations to convergence was about &num on average ) actually produced &name 's that gave a higher error rate than those obtained from &name . 
^ this is perhaps not that surprising , as only the log-likelihood of the &name 's is being maximised , not necessarily the performance of the recogniser . 
^ however , running herest for &num separate iterations improved the performance of the recogniser . 
^ the best performance was actually obtained after the first iteration of herest , but improved results were also observed after &num iterations , at which point the algorithm seemed to be converging . 
^ i was under the impression that herest and &name were basically the same algorithm ( em / bw ) , thus i 'm slightly confused as to why &name degrades performance , whereas &name enhances it . 
^ regards , 
^ &name &name 
</text_normal>
</message_body>
</message>